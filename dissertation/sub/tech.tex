\chapter{Technical Background} \label{chap:tech}
Before we attempt to train models using depth information from images, we need to understand the characteristics of them. In the scope of this project, we are going to focus on three popular models for supervised classification - support vector machine (SVM), random forest (RF) and AdaBoost (ADA), as mentioned in section \ref{ssec:classifiers}. 

Looking ahead, we should also look into $K$-means clustering, an unsupervised classification model. Rather than create a classifier using $K$-means clustering, we intend to use it to engineer our dataset. We will discuss this in \autoref{chap:methodology}.

%%%%%%%%%%%%%%%%%%%%%%
\section{Support Vector Machine (SVM)} \label{sec:tech-SVM}
% FIG - linear SVM
\begin{figure}[H]
  \centering
  \includegraphics[width=0.59\textwidth]{svm_fitting}
  \caption{\textit{This illustration shows how SVM works with linearly separable datasets and a linear kernel. The squared datapoints represent support vectors for their corresponding class.}}
  \label{fig:svm_fitting}
\end{figure}

% MATHS - linear SVM
\begin{equation} \label{eq:linear_svm}
  y(\mathbf{x}) = \mathbf{w}^T \phi(\mathbf{x}) + b
\end{equation}

The linear model above describes the objective of a two-class classification problem. Given a fixed feature-space, $\phi(\mathbf{x})$, there exists at least one pair of weight vector $\mathbf{w}$ and bias $b$ values that gives a solution that separates the input data exactly in the linear feature space. This input data is known as the training dataset, with input vectors $\mathbf{x}_1,...,\mathbf{x}_N$ each corresponding to a target $t$ with value -1 or 1, i.e. the two different classes. 'Separating exactly' means that

\begin{empheq}[left=\empheqlbrace]{align*} 
  & y(\mathbf{x}_n) > 0 \text{ for any point with } t_n = +1 \\
  & y(\mathbf{x}_n) < 0 \text{ for any point with } t_n = -1 \\
  & \text{hence, } t_n y(\mathbf{x}_n) > 0 \text{ for any point } 
\end{empheq}

If there is more than one pair of parameters that achieve the above, the pair with the smallest generalisation error should be chosen. In SVM, this is done by finding the decision boundary when the margins are maximised. A margin is the smallest perpendicular distance between the decision boundary and any datapoint. The datapoints that enable the margins to be maximised are known as the support vectors. This is illustrated in \autoref{fig:svm_fitting}.

\begin{equation} \label{eq:svm_linear_dist}
  \frac{t_n y(\mathbf{x}_n)}{||\mathbf{w}||} = \frac{t_n (\mathbf{w}^T \phi(\mathbf{x}_n) + b)}{||\mathbf{w}||}
\end{equation}

Assuming all data is classified correctly, the distance between any point $\mathbf{x}_n$ to the decision surface is given by \autoref{eq:svm_linear_dist}. Hence, the maximum margin is obtained by considering the pair of weight vector and bias that maximises the distance between the boundary and the closest datapoint $\mathbf{x}_n$, as illustrated in \autoref{eq:svm_linear_max}. As $||\mathbf{w}||$ does not depend on any datapoint, we take out the factor outside of the $\mathrm{min}$ function. 

\begin{equation} \label{eq:svm_linear_max}
  \underset{w,b}{\mathrm{argmin}} \left \{ \frac{1}{||\mathbf{w}||} \underset{n}{\mathrm{min}} \left [ t_n \left (\mathbf{w}^T \phi \left (\mathbf{x}_n \right) + b \right) \right ] \right \} 
\end{equation}

\subsection{Overlapping Classes}

% FIG - overlapping SVM
\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{svm_overlapping}
  \caption{\textit{An elaborated \autoref{fig:svm_fitting} showing slack values of points based on their location in relation to the margin and decision boundary.}}
  \label{fig:svm_overlapping}
\end{figure}
In a realistic situation, classes may overlap. If we impose a hard separation between these classes, the classifier may become too specific and not be able to predict new data. Therefore, there needs to be a trade-off between splitting the classes perfectly and misclassifying some of the datapoints. 

The slack value describes the status of a datapoint. There are three types of values as illustrated in \autoref{fig:svm_overlapping} and \autoref{tab:slack}. Correctly classified points lie on or within the margin, whereas misclassified points are on the other side of the decision boundary. Some datapoints lying on the wrong side of the margin but within its side of the boundary causes margin violation.

% TABLE: slack values
\parbox{\linewidth} {
	\centering
  \begin{tabularx}{\textwidth}{|l|X|}
    \hline
    \textbf{Slack Value}	& \textbf{Description}
    \\ \hline
    $\xi = 0$ 					  & Points that are correctly classified.
    \\ \hline
    $0 < \xi \leq 1$     	& Points that lie between the margin and the side of the hyperplane of its class.
		\\ \hline
    $\xi > 1$  						& Points that are misclassified.
    \\ \hline

  \end{tabularx}
	\captionof{table}{\textit{Slack values and their meanings.}}
	\label{tab:slack}
}

\subsubsection{The Cost Parameter ($C$)}
By allowing some datapoints to be misclassified, we can create a more generalised SVM classifier. In other words, we impose a \textbf{soft margin} through penalising these datapoints and relaxing the constraints. In a non-linear dataset, we can think of this as the smoothness of the fit. The smoother the margins, the less we take into account the exact position of all datapoints. 

\begin{equation} \label{eq:soft_margin}
	\min \left ( C \sum_{n=1}^{N} \xi_{n} + \frac{1}{2}||\mathbf{w}||^2 \right )
\end{equation}

This trade off between the margin and misclassification is controlled by the regularisation parameter, cost ($C$). The bigger $C$ is, the more correctly classified points there are, the harder the boundary becomes. As $C$ tends to infinity, we have a hard margin. Hence, we need to minimise the $C$ value in order to find our optimal margin with \autoref{eq:soft_margin}.

% TABLE: C values
\parbox{\linewidth} {
	\centering
  \begin{tabularx}{\textwidth}{|l|X|}
    \hline
    \textbf{Cost ($C > 0 $)}	  & \textbf{Description}
    \\ \hline
    Small $C$ 							    & A little constrained, large margin. 
    \\ \hline
    Large $C$  	                & A more constrained, narrower margin.
		\\ \hline
    $C \rightarrow \infty$      & A hard margin where all constraint is in place.
    \\ \hline

  \end{tabularx}
	\captionof{table}{\textit{The effects of different $C$ values on the margin.}}
	\label{tab:c_vals}
}


\subsection{Kernels}
By mapping datapoints to higher dimensional feature spaces, originally non-linearly separable datasets become separable linearly. Recall \autoref{eq:svm_linear_max}, this feature space raising is represented as $\phi \left (\mathbf{x}_n \right)$ which could be of infinite dimensions and unable to be stored. Instead, in SVM, we can represent this complex decision boundary in efficient kernel representation without computing $\phi \left (\mathbf{x}_n \right)$. 

% TABLE: available kernels
\parbox{\linewidth} {
  \centering
  \def\arraystretch{1.5}
  \begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{Kernel}     & \textbf{Mathematical Expression}
    \\ \hline
    Linear              & $k(\mathbf{x},\mathbf{x'}) = \mathbf{x}^T\mathbf{x'}$
    \\ \hline
    D-degree polynomial & $k(\mathbf{x},\mathbf{x'}) = \left(1+\mathbf{x}^T\mathbf{x'}\right)^d$ 
    \\ \hline
    Radial Basis Function (RBF) & $k(\mathbf{x},\mathbf{x'}) = \exp \left( -\gamma ||\mathbf{x}-\mathbf{x'}||^2 \right)$
		\\ \hline
    Sigmoid             & $k(\mathbf{x},\mathbf{x'}) = \tanh \left(a\mathbf{x}^T\mathbf{x'} + r\right)$
    \\ \hline
  \end{tabularx} 
  \captionof{table}{\textit{Available kernels for SVM Classifiers in the }\texttt{scikit-learn} \textit{library.}}
  \label{tab:kernel_vals}
}

\subsubsection{The Gamma Parameter ($\gamma$)}
By taking $C$ into account, we are able to create a more generalised classifier, but it would still be affected by outliers. The RBF kernel gives another parameter, $\gamma$, to control for the problem and shape of the decision boundary, as shown in \autoref{tab:kernel_vals}.

$\gamma$ defines how far the influence of each datapoint reaches. The smaller $\gamma$ is, the further it reaches. The higher the value, the more the model tries to avoid misclassification. However, if the value is too high, the decision boundary become too specific, causing overfitting (illustrated in \autoref{fig:svm_rbf_gamma}). Hence, the optimisation problem now includes the need to minimise $\gamma$ so to find the optimal decision boundary.

% FIG - SVM RBF gamma
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{svm_rbf_gamma}
  \caption{\textit{Effects of different $\gamma$ values in ascending order.}}
  \label{fig:svm_rbf_gamma}
\end{figure}


\subsection{Multiclass Extension}
Fundamentally, SVM is a binary classifier designed to split between two classes. Two approaches, namely \textit{one-versus-the-rest} and \textit{one-versus-one}, have been developed to allow SVM to work with more than two classes by combining multiple binary SVMs.

\begin{itemize}
  \item \textbf{\underline{One-versus-the-Rest}} \\ \\
    The approach trains $K$ SVM classifiers for $C_K$ classes. Each SVM classifier $y_k(\mathbf{x})$ uses the $k$\textsuperscript{th} class $C_k$ as the positive class and the rest as the negative class. However, this approach has a few issues. 

A major issue is that this causes an imbalance of classes as the positive class will be much smaller than the rest of the classes. This problem is worsened as more classes are added to the classifier, as it skews the results to the negative class, giving a false impression that the classifier has a good performance.
\\

  \item \textbf{\underline{One-versus-One}} \\ \\
Another approach is to train many SVMs with all possible pairs of classes (i.e. $K(K-1)/2$ SVM classifiers). The relevant classifiers then `vote' to return the highest voted class to give a prediction.

However, it takes significantly longer to converge than one-versus-the-rest due to the number of classifiers that need to be trained. It trains in $O(K^2N^2)$ time compared to $O(KN)$ for one-versus-the-rest. It also takes a long time to predict -- $O(K^2N_{s})$, where $N_s$ is the number of support vectors \cite{mur-book}. 

\end{itemize}

% FIG - Multiclass SVM issue 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{svm_multi_issues}
  \caption{\textit{Diagram showing ambiguity regions (green) for (a) the one-versus-the-rest approach, and (b) the one-versus-one approach (based on figure 4.2 of \protect\citeasnoun{bishop-book}).}}
  \label{fig:svm_multi_issues}
\end{figure}

Either way, it causes some ambiguity due to the separate evaluation of the classifiers. Some regions will be assigned to multiple classes due to the multiple classifier setup. \autoref{fig:svm_multi_issues} illustrates this issue, and we can see that one-versus-one has a much smaller ambiguity area than one-versus-the-rest, in the expense of a much longer training time.


%%%%%%%%%%%%%%%%%%%
\newpage
\section{Random Forest (RF)} \label{sec:tech-rf}

We now look at a rather different classification algorithm, random forests. It is an ensemble method that combines results from different decision trees to give the final classifier. We start by understanding what decision trees are.

\subsection{Decision Tree}
% FIG - Decision Tree 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{dt}
  \caption{\textit{A (classification) decision tree with two classes, A and B,  and `colour', `shape' and `size' as features (adapted from \protect\citeasnoun{mur-book}).}}
  \label{fig:dt}
\end{figure}

Decision trees, formally classification and regression trees (\textbf{CART}), obtain predictions by recursively splitting the input space and defining each region with a local model. Visually, this can be displayed as a tree, with each node representing a split by some threshold or criterion. 

Illustrated in \autoref{fig:dt}, the distribution of class labels is stored at each leaf. This gives a probability for each class satisfying a set of criteria by using the number of positive and negative samples. For instance, the criteria 'if the \textit{colour} is red and the \textit{size} is less than 10' gives a prediction for class A $p(y=1|\mathbf{x}) = 4/4 = 1$ and $p(y=2|\mathbf{x}) = 0/4 = 0$ for class B. Practically, decision trees are binary such that each node splits into 'yes' and 'no' according to some comparison criteria \cite{mur-book}.

For each point $\mathbf{x}$ to be predicted, it follows down the tree from the top, known as the root, to the resulting leaf node, according to the decision criteria learned from the training data.

\begin{equation} \label{eq:dt}
  f(\mathbf{x}) = E[y|\mathbf{x}] = \sum_{m-1}^{M} w_m I(\mathbf{x} \in R_m)
                                  = \sum_{m=1}^{M} w_m \phi(\mathbf{x}; \mathbf{v}_m)
\end{equation}

Formally, this model is defined as \autoref{eq:dt}. $R_m$ is the $m$\textsuperscript{th} region and $\mathbf{v}_m$ represents the feature used to split the node. Also, $\mathbf{w}_m$ represents the mean response in the $m$\textsuperscript{th} region. To generalise the model for classification purposes, this is replaced by the distribution of class labels as mentioned.

\begin{equation} \label{eq:abm}
  f(\mathbf{x}) = w_0 + \sum_{m=1}^{M} w_m\phi_m(\mathbf{x})
\end{equation}

This is essentially an adaptive basis-function model (ABM) (compare equations \ref{eq:dt} and \ref{eq:abm}) which tries to learn about the kernel rather than having to be specified manually like that in SVM. Without the need to estimate the kernel parameter which is computationally expensive, random forest learns much quicker than kernel methods such as SVM \cite{mur-book}. 

\subsubsection{Growing a Tree}
\begin{equation} \label{eq:dt_split_fn}
  (j*, t*) = \operatornamewithlimits{argmin}_{j \in \{1, ...,D\}} \operatornamewithlimits{min}_{t \in \tau} 
  \left( 
    cost \left(\{\mathbf{x}_i, y_i : x_{ij} \leq t \}\right) +
    cost \left(\{\mathbf{x}_i, y_i : x_{ij} > t\}\right)
  \right)
\end{equation}

We require some methods to decide which feature and what value to use for splitting. The split function in \autoref{eq:dt_split_fn} returns the best feature and best value by comparing a feature $x_{ij}$ to a numeric value $t$. This $t$ value is bounded by the feature values from the rest of the datapoints, stored as $\tau_{j}$ for each feature $j$.

As seen in the equation, we want to minimise the cost of picking this feature. Cost is measured in several ways, namely, \textit{misclassification rate}, \textit{cross-entropy} and \textit{Gini index}. We can minimise the \textit{misclassification rate}; we can maximise the information gained through the split by minimising the \textit{cross-entropy}; or, we can minimise the \textit{Gini index}, which is the expected error rate. 

Cross-entropy and Gini index are preferred because it would choose the feature that gives a pure split, i.e. contains only one class, in case of a draw in the error rate \cite{mur-book}. Such a split is more desirable as the class choice is more definite. We also need to think about how deep the tree should go. If the tree becomes too deep, it causes overfitting, which we want to avoid as discussed \cite{mur-book}. 

\subsection{Linking back to Random Forests}

% FIG - Random Forest 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{rf}
  \caption{\textit{The random forest model uses bagging to obtain a prediction by averaging the results from many decision trees.}}
  \label{fig:rf}
\end{figure}

Decision tree is known to perform worse than other classifiers due to the instability of the trees. These trees are called high variance estimators, as their structures are affected greatly by small changes. A solution is to use Random Forests. 

\subsubsection{Bagging}

\begin{equation} \label{eq:rf_ensemble}
  f(x) = \sum_{m=1}^M \frac{1}{M}f_m(x)
\end{equation}

Bagging attempts to reduce the variance by averaging the results from many noisy but approximately unbiased trees, created by a randomly selected subset of datapoints. Each tree in the forest casts a vote to decide on the predicted class through averaging the results of the trees in the forest, as illustrated in \autoref{eq:rf_ensemble}.

However, simply growing multiple trees with different sets of data would create highly correlated trees, which curbs the amount that the variance can go down. Random forests attempt to avoid this by not only randomly selecting subsets to be used, but also the features for building each tree. 

The randomness of the dataset for each tree may introduce extra bias in the forest. But, the averaging of trees usually decrease the variance, which results in a better model.


%%%%%%%%%%%%%%%%%%
\newpage
\section{Boosted Trees} \label{sec:tech-boosted}
Boosted decision trees (BST-DT) is the best performing classifier according to \possessivecite{compare-supervised} comparisons, surpassing both decision trees and random forests in terms of misclassification error. In this section, we will look into the technical details of how boosted trees work.

A notable example is AdaBoost, which we will discuss in this section. But first, we shall look at the concept of boosting. 

\subsection{Boosting}
% FIG - Random Forest 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{boosted}
  \caption{\textit{Demonstration of how boosted trees function. For each iteration, misclassified points are given a higher weight (points with a green border). This new dataset is then used to train the next weak classifier, eventually resulting in a perfectly segmented classification space. (Adapted from \protect\citeasnoun{online-vision})}}
  \label{fig:boosted}
\end{figure}

Commonly used in conjunction with decision trees, boosting attempts to boost the performance of weak learners ($\phi_m$ in \autoref{eq:abm}) by increasing the weight of misclassified datapoints after training each weak learner \cite{boosting}. The next weak learner will use the newly weighted dataset from the previous weak learner.

One criterion for this to function properly is that these trees must perform better than random (e.g. if we have 500 classes, the classifier needs to have an accuracy score greater than 1/500 = 0.002). Also, boosting is very resistant to overfitting \cite{mur-book}.

% EQ - optimisation problem of Boosting
\begin{equation} \label{eq:boost-optimise}
  \underset{f}{\mathrm{min}} \sum_{i=1}^{N} L(y_i, f(\mathbf{x}_i))
\end{equation}

Boosting aims to minimise the loss function across all $N$ samples as illustrated in \autoref{eq:boost-optimise}. Here, we consider a binary classification with $y \in \{0,1\}$, $L(y, \tilde{y})$ being the loss function and $f$ being an ABM model.

% EQ - Optimal estimate of ABM
\begin{align} \label{eq:abm-optimise}
  f^{*}(\mathbf{x}) = \underset{f(\mathbf{x})}{\mathrm{argmin}} = \mathbb{E}[Z]
\end{align}

In order to optimise for \autoref{eq:boost-optimise}, we need to find the optimal estimate for $f$, i.e. $f^{*}$. Formally, we express this as \autoref{eq:abm-optimise}, where $Z$ is the derivative of the loss function. Note that $\mathbb{E}$ means an estimation.

% EQ - f*
\begin{align}
  f_0(\mathbf{x}) &= \mathrm{arg} \underset{\gamma}{\mathrm{min}} \sum_{i=1}^{N} L(y_i,f(\mathbf{x}_i ; \gamma)) \label{eq:optimise_f_init}
  \\
  (\alpha_m,\gamma_m) &= \underset{\alpha,\gamma}{\mathrm{argmin}} \sum_{i=1}^{N} L(y_i, f_{m-1}(\mathbf{x}_i) + \alpha_m\phi(\mathbf{x}_i)) \label{eq:optimise_f_loop}
  \\
  f_m(\mathbf{x}) &= f_{m-1}(\mathbf{x}) + \alpha_m\phi(\mathbf{x}_i;\gamma) \label{eq:optimise_f_set}
\end{align}

This is a hard optimisation problem and should be dealt with sequentially with the \textbf{method forward stagewise additive modelling} \cite{mur-book}. We start by assigning the loss function of the chosen algorithm as the initial estimate by \autoref{eq:optimise_f_init}. Then, we iterate for $M$ times. At each $m$, we compute $\alpha_m$ and $\gamma_m$ by \autoref{eq:optimise_f_loop} and use them to evaluate $f$ at $m$ by \autoref{eq:optimise_f_set}, without changing any parameter at previous $m$'s. We can \textit{early stop} once performance drops, even if we haven't reached $M$ iterations. By combining $f_m$'s, we will find the resultant strong classifier. 

Note that each loss function computes \autoref{eq:optimise_f_loop} differently \cite{mur-book}. In large, there are four types of loss functions used in various boosting algorithms, notably, squared error for L2Boosting, absolute error for gradient boosting, exponential loss for AdaBoost and logloss for LogitBoost.

Now that we have seen the general details of optimising the boosting problem, we shall see how this applies to AdaBoost. 


\subsection{AdaBoost}

% EQ - AdaBoost
\begin{equation} \label{eq:ada}
  f(x) = \sum_{m=1}^{M} \alpha_m \phi_m (\mathbf{x})
\end{equation}

AdaBoost adapts the boosted algorithm by increasing the weight of each misclassified point at each iteration. Usually, deicsion trees are used as weak learners, which segment into areas and use weighted datapoints to improve accuracy, as illustrated in \autoref{fig:boosted}. \autoref{eq:ada} shows how weak learners are combined to form a strong learner $f(x)$, which follows with the foward stagewise additive modelling method.

There are two AdaBoost algorithms - \textbf{discrete AdaBoost} (\texttt{SAMME} in \texttt{scikit-learn}) and \textbf{continuous AdaBoost} (\texttt{SAMMME.R} in \texttt{scikit-learn}). Discrete AdaBoost, illustrated below, uses binary class labels from weak learners, whereas continuous AdaBoost uses probabilities instead \cite{mur-book}.

% EQ - AdaBoost optimisation problem
\begin{equation} \label{eq:ada-optimise}
  L_m(\phi) = \sum_{i=1}^{N} w_{i,m} \mathrm{exp}(-\alpha y_i \phi(\mathbf{x}_i))
\end{equation}

Again, we consider a binary classification problem here for simplicity. It can be generalised for multiclass problems easily \cite{mur-book}. At each iteration $m$, the goal is to minimise the loss function for the corresponding weak learner, $\phi_m$, which requires the learning rate parameter, $\alpha_m$. 

Here, $w_{i,m}$ is the weight applied to the dataset $i$ and $y_i \in \{-1, 1\}$ as the two binary classes. We use \{-1, 1\} because it makes it possible to use the $\mathrm{sign}$ function to obtain the class labels for discrete AdaBoost.

\subsubsection{The Learning Rate Parameter ($\alpha$)} 

% EQ - AdaBoost alpha
\begin{align}
  \alpha_m &= \frac{1}{2} \: \mathrm{log} \, \frac{1-\mathrm{err}_m}{\mathrm{err}_m}
  \label{eq:ada-alpha} \\
  \mathrm{err}_m &= \frac{\sum_{i=1}^N \: w_i \mathbb{I}(y_i \neq \phi_m(\mathbf(x)_i)}{\sum_{i=1}^{N} \: w_{i,m}}
  \label{eq:ada-alpha-err}
\end{align}


$\alpha$ is the main parameter of AdaBoost. It manages how influence each weak learner is to the resultant classifier, based on the error rate, as illustrated in \autoref{eq:ada-alpha} \cite{ada-improved}. We want to minimise this while to avoid overfitting. By using weight and the weighted samples, we obtain the error rate, as illustrated in \autoref{eq:ada-alpha-err}.






%%%%%%%%%%%%%%%%%%%
\newpage
\section{$K$-means Clustering} \label{sec:tech-kmeans}
%TODO: referencing back to lit review
%TODO: change reference to section rather than subsections
Recall that we briefly looked at unsupervised classifiers in section \ref{sec:classification}. Rather than learning from a labelled dataset, the algorithm compares values between points and creates $K$ groups, or clusters, in a multi-dimensional spacei, as specified by the user.

Later on, in Section \ref{ssec:datapoint-reduction}, we will look at how $K$-means clustering is utilised to downsize the training dataset so that we can perform tests with the aforementioned classification models. But first, let us understand the mathematical basis of $K$-means clustering.

% FIG - SVM RBF gamma
\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{k-means}
  \caption{\textit{An example of two clusters found with centroids $\mathbf{\mu}_1$ and $\mathbf{\mu}_2$ using $K$-means clustering.}}
  \label{fig:k-means}
\end{figure}

The algorithm aims to split the data into $K$ clusters where the differences of distances between the points in the group is smaller than that to the points outside of the group. 

Formally, we have to minimise the distortion measure, shown as \autoref{eq:k-means_minimise}, by finding the appropriate \{$r_{nk}$\} and \{$\mathbf{\mu}_k$\}, and through the sum squared differences between each datapoint and each $\mathbf{\mu}_k$. For each point $\mathbf{x}_n$, $r_{nk}$ is a set of binary values indicating which cluster the point belongs to. $\mathbf{\mu}_k$ is a set of vectors associated with the $k$\textsuperscript{th} cluster, representing the centres of each cluster. 

\begin{equation} \label{eq:k-means_minimise}
  J = \sum_{n=1}^{N} \sum_{k=1}^{K} r_{nk}||\mathbf{x}_n - \mathbf{\mu}_k||^2
\end{equation}

\autoref{eq:k-means_minimise} can be posed as a two-stage optimisation problem. We first find $r_{nk}$ by fixing $\mathbf{\mu}_k$ (the expectation stage, \textbf{E}) and vice versa (the maximisation stage, \textbf{M}):

\newpage
\begin{itemize}
  % finding optimal r
  \item \textbf{\underline{Stage 1: Finding Optimal $r_{nk}$}} \\ \\
	\begin{equation} \label{eq:k-means_r_nk}
		r_{nk} =
		\begin{cases}
			1 & \text{if } k=\mathrm{argmin}_j ||\mathbf{x}_n - \mathbf{\mu}_j||^2\\
			0 & \text{otherwise}
		\end{cases}
  \end{equation} \\
	
  Staring with some randomly selected datapoints from the dataset as $\mathbf{\mu}_k$, we compute the sum squared differences between each point and the proposed cluster centres. We assign each datapoint to its corresponding closest cluster centres. In the end, we will have a set of datapoints that belong to a cluster centre, hence finding groups in the data.
\\
  % finding optimal mu
  \item \textbf{\underline{Stage 2: Finding Optimal $\mathbf{\mu}_k$}} \\ \\ 
    \begin{align} \label{eq:k-means_mu_min}
    \begin{split}
    2\sum_{n=1}^{N}r_{nk}(\mathbf{x}_n - \mathbf{\mu}_k) &=  0 \\ 
    \mathbf{\mu}_k &= \frac{\sum_n r_{nk}\mathbf{x}_n}{\sum_n r_{nk}}
    \end{split}
	\end{align} \\
  
  These potential clusters might not be optimised to minimise \autoref{eq:k-means_minimise}. So, We have to do so by computing the derivative with respective to $\mathbf{\mu}_k$ as illustrated in \autoref{eq:k-means_mu_min}. Also, notice that by finding $\mathbf{\mu}_k$, we find out that they are the means of all the datapoints assigned to them, i.e. the centroids of the groups.
\end{itemize}

The two stages are repeated until the model converges, meaning that no further reassignment happens. The end result will be $K$ groups governed by the mean values of their corresponding cluster. Note that these means may not necessarily be concrete datapoints from the dataset, but the actual mean of the corresponding clusters. 

Recall in \autoref{ssec:classifiers}, we mentioned about the Expectation Maximisation algorithm. Usually used in conjunction with DBSCAN, this can be used by most clustering problems like $K$-means as we discussed above.

%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Summary}
So far, we have learnt that SVM contains a main parameter, $C$, which controls the trade-off between the margin and misclassification. With an RBF kernel, we can control the shape of the decision boundary through the $\gamma$ parameter. By finding the appropriate parameters, we can obtain an optimal SVM classifier that gives the best possible accuracy with the given dataset.

Also, we have learnt how the combination of randomised data and features in random forests give a much better accuracy and prediction power than a single decision tree. Comparing with SVM, we have learnt that random forests can be trained faster. Rather than having to estimate the correct parameters for the base kernel, RF learns it themselves. However, we do have to choose an appropriate height of the tree and the number of the trees to train to avoid overfitting.

Boosted algorithms such as AdaBoost use a collection of weak classifiers and weight up ill-classified datapoints to reduce the bias of the resultant classifier, whereas RF averages the results of more complicated trees over a random set of datapoints and features. This means that boosted algorithms are theoretically more efficient, as they require fewer and simplier trees.

We shall now discuss the overall process used for training these algorithms.
