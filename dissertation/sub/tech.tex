\documentclass[dissertation.tex]{subfiles}
\begin{document}

\chapter{Technical Background}
Before we attempt training models using depth information from images, we need to understand the characteristics of some of the models. In the scope of this project, we should focus on the two popular models for supervised machine learning - support vector machine and random forests, as mentioned in $(lit review section)$. 

Looking ahead, we should also look into K-means Clustering, an unsupervised training model. It is not used for fitting the depth patches we intend to use, but as a way to perform our feature engineering, which will be discussed in \autoref{chap:methodology}.

\section{Support Vector Machine (SVM)}

% FIG - linear SVM
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{svm_fitting}
  \caption{\textit{This illustration shows how SVM works with linearly separable datasets. The squared datapoints represent support vectors for their corresponding class.}}
  \label{fig:svm_fitting}
\end{figure}

% MATHS - linear SVM
\begin{equation} \label{eq:linear_svm}
  y(\mathbf{x}) = \mathbf{w}^T \phi(\mathbf{x}) + b
\end{equation}

The linear model above describes the objective of a two-class classification problem. Given a fixed feature-space, $\phi(\mathbf{x})$, there exists at least one pair of weight vector $\mathbf{w}$ and bias $b$ values that gives a solution that separates the input data exactly in the linear feature space. This input data is known as the training dataset, with input vectors $\mathbf{x}_1,...,\mathbf{x}_N$ each corresponding to a target $t$ with value -1 or 1, i.e. the two different classes. 'Separating exactly' means that

\begin{empheq}[left=\empheqlbrace]{align*} 
  & y(\mathbf{x}_n) > 0 \text{ for any point with } t_n = +1 \\
  & y(\mathbf{x}_n) < 0 \text{ for any point with } t_n = -1 \\
  & \text{hence, } t_n y(\mathbf{x}_n) > 0 \text{ for any point } 
\end{empheq}

If there are more than one pairs of parameters that achieve the above, the pair with the smallest generalisation error should be chosen. In SVM, this is done by finding the decision boundary with the margin maximised. The margin is the smallest perpendicular distance between the decision boundary and any datapoint. The datapoints that enables the margin to be maximised are known as the support vectors. This is illustrated in \autoref{fig:svm_fitting}.

\begin{equation} \label{eq:svm_linear_dist}
  \frac{t_n y(\mathbf{x}_n)}{||\mathbf{w}||} = \frac{t_n (\mathbf{w}^T \phi(\mathbf{x}_n) + b)}{||\mathbf{w}||}
\end{equation}

Assuming all data is classified correctly, the distance between any point $\mathbf{x}_n$ to the decision surface is given by \autoref{eq:svm_linear_dist}. Hence, the maximum margin is obtained by considering the pair of weight vector and bias that maximises the distance between the boundary and the clostest datapoint $\mathbf{x}_n$, as illustrated in \autoref{eq:svm_linear_max}. As $||\mathbf{w}||$ does not depend on any datapoint, we take out the factor outside of the $\mathrm{min}$ function. 

\begin{equation} \label{eq:svm_linear_max}
  \underset{w,b}{\mathrm{argmin}} \left \{ \frac{1}{||\mathbf{w}||} \underset{n}{\mathrm{min}} \left [ t_n \left (\mathbf{w}^T \phi \left (\mathbf{x}_n \right) + b \right) \right ] \right \} 
\end{equation}

\subsection{Overlapping Classes}

% FIG - overlapping SVM
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{svm_overlapping}
  \caption{\textit{An elaborated \autoref{fig:svm_fitting} showing slack values of points based on its location in relation to the margin and decision boundary.}}
  \label{fig:svm_overlapping}
\end{figure}
In a realistic situation, classes may overlap. If we impose a hard separation between these classes, the classifier may become too specific and not be able to predict new data. Therefore, there needs to be a trade-off between splitting the classes and misclassifying some of the datapoints. 

The slack value describes the status of a datapoint. There are three types of values as illustrated in \autoref{fig:svm_overlapping} and \autoref{tab:slack}. Correctly classified points lie on or within the margin, whereas misclassified points are on the other side of the hyperplane. Some datapoints lie on the wrong side of the margin but within its side of the hyperplane causing margin violation.

% TABLE: slack values
\parbox{\linewidth} {
	\centering
  \begin{tabularx}{\textwidth}{|l|X|}
    \hline
    \textbf{Slack Value}		& \textbf{Description}
    \\ \hline
    $\xi = 0$ 							& Points that are correctly classified.
    \\ \hline
    $0 < \xi \leqslant 1$  	& Points that lie between the margin and the side of the hyperplane of its class.
		\\ \hline
    $\xi > 1$  							& Points that are misclassified.
    \\ \hline

  \end{tabularx}
	\captionof{table}{\textit{Slack values and their meanings.}}
	\label{tab:slack}
}

\subsubsection{The Cost Parameter ($C$)}
By allowing some datapoints to be misclassified, we can create a more generalised SVM classifier. In other words, we impose a \textbf{soft margin} through penalising these datapoints and relaxing the contraints. In a non-linear dataset, we can think of this as the smoothness of the fit. The smoother the decision boundary, the less we take into account the exact position of all datapoints. 

\begin{equation} \label{eq:soft_margin}
	\min \left ( C \sum_{n=1}^{N} \xi_{n} + \frac{1}{2}||\mathbf{w}||^2 \right )
\end{equation}

This trade off between the margin and misclassification is controlled by the regularisation parameter, cost ($C$). The bigger $C$ is, the more correctly classified points there are, the harder the boundary becomes. As $C$ tends to infinity, we have a hard margin. Hence, we need to minimise the $C$ value in order to find our optimal margin with \autoref{eq:soft_margin}.

% TABLE: C values
\parbox{\linewidth} {
	\centering
  \begin{tabularx}{\textwidth}{|l|X|}
    \hline
    \textbf{Cost ($C > 0 $)}	  & \textbf{Description}
    \\ \hline
    Small $C$ 							    & Little contrained, large margin. 
    \\ \hline
    Large $C$  	                & A more contrained, narrower margin.
		\\ \hline
    $C \rightarrow \infty$      & A hard margin where all contraint is in place.
    \\ \hline

  \end{tabularx}
	\captionof{table}{\textit{The effects of the values of $C$ on the margin.}}
	\label{tab:c_vals}
}


\subsection{Kernels}
By mapping datapoints to higher dimensional feature spaces, originally non-linearly separable datasets usually become separable linearly. Recall \autoref{svm_linear_max}, this feature space raising is represented as $\phi \left (\mathbf{x}_n \right)$ which could be of infinite dimensions and unstorable. Instead, in SVM, we can represent this complex decision boundary in efficient kernel representation without. 

% TABLE: available kernels
\parbox{\linewidth} {
  \centering
  \def\arraystretch{1.5}
  \begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{Kernel}     & \textbf{Mathematical Expression}
    \\ \hline
    Linear              & $k(\mathbf{x},\mathbf{x'}) = \mathbf{x}^T\mathbf{x'}$
    \\ \hline
    D-degree polynomial & $k(\mathbf{x},\mathbf{x'}) = \left(1+\mathbf{x}^T\mathbf{x'}\right)^d$ 
    \\ \hline
    Radial Basis Function (RBF) & $k(\mathbf{x},\mathbf{x'}) = \exp \left( -\gamma ||\mathbf{x}-\mathbf{x'}||^2 \right)$
		\\ \hline
    Sigmoid             & $k(\mathbf{x},\mathbf{x'}) = \tanh \left(a\mathbf{x}^T\mathbf{x'} + r\right)$
    \\ \hline
  \end{tabularx} 
  \captionof{table}{\textit{Available SVM Classifier kernels in the }\texttt{scikit-learn} \textit{library.}}
  \label{tab:kernel_vals}
}

RBF is a useful and popular kernel.

\subsubsection{The Gamma Parameter ($\gamma$)}
By taking $C$ into account, we are able to create a more generalised classifier, but it would still be affected by outliers. The RBF kernel gives another parameter, $\gamma$, to control for the problem and shape of the decision boundary, as shown in \autoref{tab:kernel_vals}.

$\gamma$ defines how far the influence of each datapoint reaches. The smaller $\gamma$ is, the further it reaches. The higher the value, the more the model tries to avoid misclassification. However, if the value is too high, the decision boundary become too specific, causing overfitting. Hence, the optimisation problem now includes the need to minimise $\gamma$ such that it finds the optimal decision boundary, as illustrated in \autoref{fig:svm_rbf_gamma}.


% FIG - SVM RBF gamma
\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{svm_rbf_gamma}
  \caption{\textit{Effects of different $\gamma$ values in ascending order.}}
  \label{fig:svm_rbf_gamma}
\end{figure}

\newpage
\section{Random Forests (RF)}
We now look at another classificiation algorithm, random forests.

\newpage
\section{K-means Clustering}
%TODO: referencing back to lit review
Recall that we looked briefly at unsupervised in \autoref{sssec:unsupervised_overview} and \ref{something}. Rather than learning from a labelled dataset, the algorithm learns from the characteristics in the data and assigns labels to the groups it identifies.

Later on, in \autoref{sec:method_k-means}, we will discuss our application of K-means clustering for downsizing our dataset for training the aforementioned calssifier models in a rather unusual manner. 

\end{document}
