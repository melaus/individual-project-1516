\chapter{Introduction} \label{chap:introduction}
Machine learning has been an interest since the early days of the invention of computers. One active area of research of the application of machine learning is computer vision. It aims to learn from images by finding distinctive characteristics in the underlying numerical or categorical values through methods like curve fitting and splitting up the data.  For humans, we understand the context of an image so that we can still recognise objects in it even if they are of different colour or made from different materials, but it is a challenging problem for computers. 

Images are inherently difficult to model as they have complicated distribution in the data level. An orange in different images may appear different due to the environment it is in, the angle of which it is taken from or colour temperature of the scene. Hence, simply using pixel (colour) information may not make the cut.

Nowadays, some cameras are capable of capturing not only colours, but also depth information to show where an object sits in a 3-dimensional space. One particular interesting project is Kinect Fusion, where it fuses images together to create a 3-dimensional reconstruction of the scene and represent it in a 3-dimensional space.

In fact, research, such as SemanticPaint \cite{semantic-paint}, made use of this new source of information to create a virtual/ augmented reality experience. \citeasnoun{semantic-paint} provided a way to label objects and applied the same effect on other similar objects in a scene in real-time. However, these recognised objects are not stored. Can we find a generalised model using depth information to correctly predict objects in a scene? This way, we could use this information to enable more interesting virtual/ augmented reality experience. 

A good use-case would be a real-time system that suggests what can be placed on top of a given recognised object. For instance, the system might suggest that `bowls' and `cups' can be placed on top of a table when a table top is recognised, and `pots' and `pans' when a stove is recognised. 

Such recognition is a classification problem. We want to find an optimised decision boundary that splits the data into regions in order to obtain a model that can predict unseen objects. Together with a complex distribution, this optimisation problem becomes even more complicated and requires careful consideration. This contributes to an accuracy and speed trade-off which we will look into in greater detail.


\section{Aim}
There are several aims with this project. For one, we want to find out how popular classification algorithms perform in real-world applications. Formally, these models are very robust with techniques to avoid common issues such as overfitting (taking noise in the training data as features for the whole datasets). But, it is hard to know how they perform with large and complex datasets. The raw dataset that we are basing this project on will contain many images hence many possible datapoints. In \autoref{chap:methodology}, we will discuss our approach in dealing with such situation.

We also want to investigate the usefulness of depth data alone in predicting objects in a scene. Could we obtain a classifier with high enough accuracy that can predict objects in an image to good quality? With a high accuracy classifier, we can integrate it into the aforementioned use-case in conjunction with other computer vision techniques such as segmentation. 

To limit the scope of the project, we will focus on training supervised classification models, which we will discuss briefly in \autoref{sec:classification} and in detail in \autoref{chap:tech}.

\section{Evaluating Success}
In order to evaluate how well our classifiers perform, we use various quantitative approaches, including accuracy and precision-recall rates. With these values, we can examine whether we have trained a useful classifier with good prediction power. 

Also, we want to ensure that we find the best decision boundary fit while avoiding overfitting. An overfitted classifier does not possess any meaningful prediction power, even though it might show impressive results in hindsight. We want to avoid being too specific by using every detail of the training dataset without compromise, otherwise we will end up with a classifier that is tailored only to this dataset. 

We will discuss all these concepts in detail in the upcoming chapters.
